{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Project 2\n",
    "## Arman Rostami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we'll try to design an agent to play a game like Checkers using Minimax algorithm. Next, we'll use Alphaâ€“beta pruning to improve speed by decreasing the number of nodes that are going to be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax algorithm is used in Aversarial search problems where there are two agents competing each other and each agent tries to plan ahead based on the world and other agent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of Agents in Minimax algorithm : Min agent and Max agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min agent is the agent which tries to minimize other agent's value by selecting the best move possible, Max agent tries to maximize it's value by selecting the best move respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax tries to find the best move for agent which may maximize it's value or minimize other agent's value by selecting a move based on the state's successor's values which may be maximum move or minimum move based on agent's type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It computes game tree by generating each state's successor states using a recursive approach. By using a complete depth-first exploration of the game tree, if the maximum depth of the tree is m and there are b legal moves at each point, then the time complexity of the minimax algorithm is $O(b ^ m)$. The space complexity is $O(bm)$ for an\n",
    "algorithm that generates all actions at once, or $O(m)$ for an algorithm that generates actions one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on type of the agent's node in game tree, node's value is the best reachable value from expanding node to reach the leaves. leaves values correspond to one agent's value which can be evaluated by an utility function which assigns an value to leaf as one agent's score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game tree can be very large and may be difficult to expand whole game tree to reach leaves and find nodes values because of time limits or resource limits. One solution is to use an evaluation function to evaluate values for non-terminal nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll search to a limited depth by using Depth-limited search. When reaching a non-terminal node, value of evaluation function of that node is returned as it's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation function chosen in this game uses two other evaluation functions to evaluate nodes values. It uses each evaluation function with a uniform weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First evalution function computes number of Max agent player and Min agent player possible moves and returns it's difference. Having more possible moves may increase chance of winning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second evaluation function examines number of players pieces. It returns difference between Max agent pieces and Min agent pieces. A state with more agent's pieces is a better state for that agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows initialize function of MinimaxPlayer class which sets remaining search depth and board and agent's side to also represent the current state of player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(self, side, depth, board = None):\n",
    "    self.side = side\n",
    "    self.depth = depth\n",
    "    self.name = \"Minimax\"\n",
    "    if board != None:\n",
    "        self.board = copy.deepcopy(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following codes in MinimaxPlayer class to calculate evalution function. $\\texttt{getEvaluation1}$ is the first evaluation function and $\\texttt{getEvaluation2}$ is the second evaluation function which are used by $\\texttt{getEvaluation}$ function with a uniform weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluation1(self, board):\n",
    "    bMoves = self.generateMoves(board, 'B')\n",
    "    wMoves = self.generateMoves(board, 'W')\n",
    "    return len(bMoves) - len(wMoves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluation2(self, board):\n",
    "    bPieces = 0\n",
    "    wPieces = 0\n",
    "    for row in board:\n",
    "        for piece in row:\n",
    "            if piece == 'B':\n",
    "                bPieces += 1\n",
    "            elif piece == 'W':\n",
    "                wPieces += 1\n",
    "    return bPieces - wPieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluation(self, board):\n",
    "    return self.getEvaluation1(board) + self.getEvaluation2(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows implementation of $\\texttt{getMove}$ function which returns best move for Minimax player based on it's type. If agent is the Max agent, It selects a move which makes the maximum value otherwise it returns best move which makes the minimum value of other agent. It uses $\\texttt{getValue}$ function to get value of successor nodes values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMove(self, board):\n",
    "    moves = self.generateMoves(board, self.side)\n",
    "    values = []\n",
    "    for move in moves:\n",
    "        child = self.createChild(board)\n",
    "        child.makeMove(self.side, move)\n",
    "        childValue = child.getValue(child.board)\n",
    "        values.append(childValue)\n",
    "    if len(moves) == 0:\n",
    "        return []\n",
    "    elif self.side == 'B':\n",
    "        return moves[values.index(max(values))]\n",
    "    elif self.side == 'W':\n",
    "        return moves[values.index(min(values))]\n",
    "    else:\n",
    "        raise GameError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows implementation of $\\texttt{getValue}$ function which returns node's value. If current node is Max agent's turn, It returns maximum value of current node's successor's values which are other agent's turn who tries to minimize Max agent's value. On Min agent's turn, It returns minimum value of current node's successors which are other agent's turn who tries to maximize it's value. When reaching a win or lose state or a state with zero depth, value of $\\texttt{getEvaluation}$ for that node is returned. This function uses $\\texttt{createChild}$ function to generate a player's successor state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValue(self, board):\n",
    "    if self.depth == 0:\n",
    "        return self.getEvaluation(board)\n",
    "    moves = self.generateMoves(board, self.side)\n",
    "    if len(moves) == 0:\n",
    "        return self.getEvaluation(board)\n",
    "    value = -math.inf if self.side == 'B' else math.inf\n",
    "    for move in moves:\n",
    "        child = self.createChild(board)\n",
    "        child.makeMove(self.side, move)\n",
    "        if self.side == 'B':\n",
    "            value = max(value, child.getValue(child.board))\n",
    "        else:\n",
    "            value = min(value, child.getValue(child.board))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows implementation of $\\texttt{createChild}$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChild(self, board):\n",
    "    child = MinimaxPlayer(self.size)\n",
    "    childSide = self.opponent(self.side)\n",
    "    child.initialize(childSide, self.depth - 1, board)\n",
    "    return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning is an algorithm used to reduce the number of nodes that have to be evaluated to reach the optimal solution. Pruning works by blocking the evaluation of nodes whose leaf nodes would give worse results compared to the one that was previously examined. We'll use Alpha-beta pruning algorithm to prune Minimax tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-beta Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha-beta pruning tries to decrease number of nodes evaluated by Minimax algorithm using two parameters: alpha and beta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha corresponds to Max agent's best option on path to root and beta corresponds to Min agent's best option on path to root respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max agent tries to find the maximum value from it's node successors which are Min agents. By having the best Max option so far as alpha, expanding nodes with value equal or less than alpha is not necessary because it won't be selected by Max agent and since Min agent tries to find minimum value of it's node successors, expanding can be stopped. Same logic goes to Min agent which tries to find the minimum value from it's node successors. By having the best value of Min agent's node as beta, we can stop expanding nodes on Max node with value equal or more than beta because this node won't be seleted because there's a better node which can be selected by the Min agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha-beta pruning selectes the same moves as Minimax player except in equal value states which alpha-beta selectes the first equal node if other successors are pruned. This property depends on implementation of Alpha-beta pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-beta Pruning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaBetaPlayer class is implemented to run Minimax with Alpha-beta pruning. It almost has same functions as MinimaxPlayer class except $\\texttt{getMove}$ and $\\texttt{getValue}$ functions which are modified to support Alpha-beta pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows implementation of AlphaBetaPlayer's $\\texttt{getMove}$ function. This function updates alpha and beta based on Agent's type and calls getValue with alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMove(self, board):\n",
    "    alpha = -math.inf\n",
    "    beta = math.inf\n",
    "    moves = self.generateMoves(board, self.side)\n",
    "    values = []\n",
    "    for move in moves:\n",
    "        child = self.createChild(board)\n",
    "        child.makeMove(self.side, move)\n",
    "        childValue = child.getValue(child.board, alpha, beta)\n",
    "        if self.side == 'B':\n",
    "            alpha = max(alpha, childValue)\n",
    "        else:\n",
    "            beta = min(beta, childValue)\n",
    "        values.append(childValue)\n",
    "    if len(moves) == 0:\n",
    "        return []\n",
    "    elif self.side == 'B':\n",
    "        return moves[values.index(max(values))]\n",
    "    elif self.side == 'W':\n",
    "        return moves[values.index(min(values))]\n",
    "    else:\n",
    "        raise GameError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows implementation of $\\texttt{getValue}$ function. This function prune unnecessary nodes by considering agent's type and alpha and beta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValue(self, board, alpha, beta):\n",
    "    if self.depth == 0:\n",
    "        return self.getEvaluation(board)\n",
    "    moves = self.generateMoves(board, self.side)\n",
    "    if len(moves) == 0:\n",
    "        return self.getEvaluation(board)\n",
    "    value = -math.inf if self.side == 'B' else math.inf\n",
    "    for move in moves:\n",
    "        child = self.createChild(board)\n",
    "        child.makeMove(self.side, move)\n",
    "        if self.side == 'B':\n",
    "            value = max(value, child.getValue(child.board, alpha, beta))\n",
    "            if value >= beta:\n",
    "                return value\n",
    "            alpha = max(alpha, value)\n",
    "        else:\n",
    "            value = min(value, child.getValue(child.board, alpha, beta))\n",
    "            if value <= alpha:\n",
    "                return value\n",
    "            beta = min(beta, value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on tests time and agent's winnings the best option for depth in MinimaxPlayer is chosen to be 3 and on AlphaBetaPlayer it's 4. The following table shows result for 5 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| First Player    | Second Player | First Player Depth | First Player winings | Average First Player Move Selection Time (s)|\n",
    "| :----------:    | :-----------: | :----------------: | :------------------: | :------------------------------: |\n",
    "| MinimaxPlayer   | Random Player | 3                  | 5 / 5                | 0.0370                           |\n",
    "| AlphaBetaPlayer | Random Player | 3                  | 5 / 5                | 0.0198                           |\n",
    "| MinimaxPlayer   | Random Player | 4                  | 5 / 5                | 0.3953                           |\n",
    "| AlphaBetaPlayer | Random Player | 4                  | 5 / 5                | 0.0739                           |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
